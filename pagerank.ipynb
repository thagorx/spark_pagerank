{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, array, lit, udf, collect_list, size, explode\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "from pyspark.sql.types import ArrayType, FloatType, BooleanType, StructType, StructField, IntegerType\n",
    "import random\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIT\n",
    "# https://stanford.edu/~rezab/dao/notes/Partitioning_PageRank.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_graph(size):\n",
    "    r_graph = [(random.randint(1, size), random.randint(1, size))\n",
    "               for i in range(size)]\n",
    "    # a tenth of edges are edges to None\n",
    "    r_graph += [(random.randint(size+1, size*2), None)\n",
    "                for i in range(int(size/10))]\n",
    "    return r_graph\n",
    "\n",
    "\n",
    "def rank_dist(link_list, rank):\n",
    "    # link_list contains the node IDs this node has edges to\n",
    "    # rank is the current rank of the node\n",
    "    len_link_list = len(link_list)\n",
    "\n",
    "    if len_link_list > 0:\n",
    "        rank = rank / len_link_list\n",
    "        r_list = [(x, rank) for x in link_list]\n",
    "    else:\n",
    "        # can't emit None so if there are no known links -1 is emited instead\n",
    "        r_list = [(-1, rank)]\n",
    "\n",
    "    return r_list\n",
    "\n",
    "\n",
    "inner_schema = StructType([\n",
    "    StructField(\"uri_id\", IntegerType(), False),\n",
    "    StructField(\"rank\", FloatType(), False)\n",
    "])\n",
    "\n",
    "ranks_dist_udf = udf(rank_dist, ArrayType(inner_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(edges_df, alpha=0.15, convergence=0.01):\n",
    "    \"\"\"\n",
    "    The function takes an spark dataframe as `edges_df` containing all edges (source to destination)\n",
    "    of the graph and calculates a pagerank value for all sources (nodes). It runs until `convergence` is achieved\n",
    "    It can deal with destinations not found as sources and with dangling edges. \n",
    "    The dataframe needs to be formated as follows with `src` and `dst` as the column names\n",
    "\n",
    "    +---+----+\n",
    "    |src| dst|\n",
    "    +---+----+\n",
    "    |  1|   1|\n",
    "    |  1|   3|\n",
    "    |  3|   4|\n",
    "    |  2|   5|\n",
    "    |  6|null|\n",
    "    +---+----+\n",
    "\n",
    "    alpha defaults to 0.15 and convergence to 0.01\n",
    "    \"\"\"\n",
    "\n",
    "    ranks_df = edges_df\\\n",
    "        .groupby('src')\\\n",
    "        .agg(collect_list('dst'))\\\n",
    "        .withColumnRenamed('src', 'uri_id')\\\n",
    "        .withColumnRenamed('collect_list(dst)', 'dst_list')\n",
    "\n",
    "    ranks_df = ranks_df.withColumn('rank', lit(1.0))\n",
    "\n",
    "    n_nodes = ranks_df.count()\n",
    "    i = 0\n",
    "\n",
    "    while True:\n",
    "\n",
    "        print(f'##### Itteration:\\t{i} #####')\n",
    "        # chaching the dataframe in the beginning makes the pagerank faster\n",
    "        ranks_df.cache()\n",
    "\n",
    "        # first we distribute the current rank to all the linked nodes\n",
    "        ranks_one_df = ranks_df.withColumn(\n",
    "            'link_map_pr', ranks_dist_udf('dst_list', 'rank'))\n",
    "        ranks_one_df = ranks_one_df.select(\n",
    "            explode('link_map_pr').alias('exploded'))\n",
    "        ranks_one_df = ranks_one_df\\\n",
    "            .withColumn('dst_id', ranks_one_df['exploded'].getItem('uri_id'))\\\n",
    "            .withColumn('rank_i', ranks_one_df['exploded'].getItem('rank'))\\\n",
    "            .drop(ranks_one_df['exploded'])\n",
    "\n",
    "        ranks_one_df = ranks_one_df\\\n",
    "            .groupby('dst_id')\\\n",
    "            .sum('rank_i')\\\n",
    "            .withColumnRenamed('sum(rank_i)', 'rank_i')\n",
    "\n",
    "        # next we have to deal with dangling nodes and nodes that edges to unkown nodes not present in the graph\n",
    "        # in the end all of this gets also collected into alpha and distrbuted to all nodes in the graph\n",
    "        # full outer join because you dont want to lose nodes that either have no know input or no know output\n",
    "        ranks_df = ranks_df\\\n",
    "            .join(ranks_one_df, ranks_df['uri_id'] == ranks_one_df['dst_id'], 'outer')\\\n",
    "            .drop('dst_id')\n",
    "\n",
    "        dangling_rank = ranks_df\\\n",
    "            .filter(ranks_df.uri_id.isNull())\\\n",
    "            .select(spark_sum('rank_i'))\\\n",
    "            .first()[0]\n",
    "\n",
    "        ranks_df = ranks_df.filter(ranks_df.uri_id.isNotNull())\n",
    "\n",
    "        # because dangling is handeled like a link to every node dangling also needs to devalued by alpha!\n",
    "        if dangling_rank:\n",
    "            dist_alpha = ((dangling_rank/n_nodes)*(1-alpha)) + alpha\n",
    "        else:\n",
    "            dist_alpha = alpha\n",
    "        print(f'dangling sum:\\t{dangling_rank}')\n",
    "        print(f'alpha dist :\\t{dist_alpha}')\n",
    "\n",
    "        sum_alpha_and_pr_udf = udf(lambda x: (\n",
    "            x * (1-alpha)) + dist_alpha, FloatType())\n",
    "        ranks_df = ranks_df.na.fill(0, ['rank_i'])\n",
    "        ranks_df = ranks_df.withColumn(\n",
    "            'rank_i', sum_alpha_and_pr_udf('rank_i'))\n",
    "\n",
    "        # test for convergence\n",
    "        convergence_udf = udf(lambda rank_i, rank: abs(\n",
    "            rank_i - rank) <= convergence, BooleanType())\n",
    "        ranks_df = ranks_df.withColumn(\n",
    "            'convergence', convergence_udf('rank', 'rank_i'))\n",
    "        count_not_converged = ranks_df.filter(\n",
    "            ranks_df.convergence == False).count()\n",
    "        ranks_df = ranks_df.drop('convergence').drop(\n",
    "            'rank').withColumnRenamed('rank_i', 'rank')\n",
    "\n",
    "        # dataframe needs to be checkpointed here to truncate the logic path\n",
    "        # otherwise it would grow larger than the avalaible memory\n",
    "        ranks_df = ranks_df.checkpoint()\n",
    "\n",
    "        if count_not_converged == 0:\n",
    "            print(\"converged\")\n",
    "            break\n",
    "        else:\n",
    "            print(f'nodes not yet converged {count_not_converged}')\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return ranks_df.drop('dst_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    input_base_path = '../data/results/'\n",
    "    webgraph_path = input_base_path + 'webgraph_v2_without_null/'\n",
    "    pagerank_out_path = input_base_path + 'pagerank_df_parquet/'\n",
    "\n",
    "    random_graph_size = 10\n",
    "    graph_path = None\n",
    "    output_path = None\n",
    "    chekpoint_dir = '../data/checkpoints'\n",
    "\n",
    "    spark.sparkContext.setCheckpointDir(chekpoint_dir)\n",
    "\n",
    "    if graph_path:\n",
    "        graph_df = spark.read.parquet(webgraph_path).cache()\n",
    "    else:\n",
    "        graph_df = spark\\\n",
    "            .createDataFrame(create_random_graph(random_graph_size))\\\n",
    "            .withColumnRenamed('_1', 'src')\\\n",
    "            .withColumnRenamed('_2', 'dst')\n",
    "\n",
    "        print(graph_df.show())\n",
    "\n",
    "    ranked_graph_df = run(graph_df)\n",
    "    print(ranked_graph_df.show())\n",
    "\n",
    "    if output_path:\n",
    "        ranked_graph_df.write.parquet(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
