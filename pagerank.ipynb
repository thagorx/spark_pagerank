{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, array, lit, udf, collect_list, size, explode\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "from pyspark.sql.types import ArrayType, FloatType, BooleanType, StructType, StructField, IntegerType\n",
    "import random\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todos:\n",
    " - make it pep8 conform\n",
    " - more code comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession\\\n",
    "#         .builder\\\n",
    "#         .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIT\n",
    "# https://stanford.edu/~rezab/dao/notes/Partitioning_PageRank.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_graph(size):\n",
    "    r_graph = [(random.randint(1,size),random.randint(1,size)) for i in range(size)]\n",
    "    # a tenth of edges are edges to None \n",
    "    r_graph += [(random.randint(size+1,size*2),None) for i in range(int(size/10))]\n",
    "    return r_graph\n",
    "    \n",
    "def rank_dist(link_list,rank):\n",
    "    # link_list contains the node IDs this node has edges to \n",
    "    # rank is the current rank of the node\n",
    "    len_link_list = len(link_list)\n",
    "    \n",
    "    if  len_link_list > 0:\n",
    "        rank = rank / len_link_list\n",
    "        r_list = [(x,rank) for x in link_list]\n",
    "    else:\n",
    "        # can't emit None so if there are no known links -1 is emited instead\n",
    "        r_list = [(-1,rank)]\n",
    "    \n",
    "    return r_list\n",
    "\n",
    "\n",
    "inner_schema = StructType([\n",
    "    StructField(\"uri_id\", IntegerType(), False),\n",
    "    StructField(\"rank\", FloatType(), False)\n",
    "])\n",
    "\n",
    "ranks_dist_udf = udf(rank_dist,ArrayType(inner_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(edges_df, alpha=0.15, convergence=0.01):\n",
    "    \n",
    "    \"\"\"\n",
    "    The function takes an spark dataframe as `edges_df` containing all edges (source to destination)\n",
    "    of the graph and calculates a pagerank value for all sources (nodes). It runs until `convergence` is achieved\n",
    "    It can deal with destinations not found as sources and with dangling edges. \n",
    "    The dataframe needs to be formated as follows with `src` and `dst` as the column names\n",
    "    \n",
    "    +---+----+\n",
    "    |src| dst|\n",
    "    +---+----+\n",
    "    |  1|   1|\n",
    "    |  1|   3|\n",
    "    |  3|   4|\n",
    "    |  2|   5|\n",
    "    |  6|null|\n",
    "    +---+----+\n",
    "    \n",
    "    alpha defaults to 0.15 and convergence to 0.01\n",
    "    \"\"\"\n",
    "\n",
    "    ranks_df = edges_df\\\n",
    "                .groupby('src')\\\n",
    "                .agg(collect_list('dst'))\\\n",
    "                .withColumnRenamed('src','uri_id')\\\n",
    "                .withColumnRenamed('collect_list(dst)','dst_list')\n",
    "\n",
    "    ranks_df = ranks_df.withColumn('rank',lit(1.0))\n",
    "    \n",
    "    n_nodes = ranks_df.count()\n",
    "    i = 0\n",
    "\n",
    "    while True:    \n",
    "\n",
    "        print(f'##### Itteration:\\t{i} #####')\n",
    "        # chaching the dataframe in the beginning makes the pagerank faster \n",
    "        ranks_df.cache()\n",
    "\n",
    "        # first we distribute the current rank to all the linked nodes\n",
    "        ranks_one_df = ranks_df.withColumn('link_map_pr',ranks_dist_udf('dst_list','rank'))\n",
    "        ranks_one_df = ranks_one_df.select(explode('link_map_pr').alias('exploded'))\n",
    "        ranks_one_df = ranks_one_df\\\n",
    "                        .withColumn('dst_id', ranks_one_df['exploded'].getItem('uri_id'))\\\n",
    "                        .withColumn('rank_i', ranks_one_df['exploded'].getItem('rank'))\\\n",
    "                        .drop(ranks_one_df['exploded'])\n",
    "\n",
    "        ranks_one_df = ranks_one_df\\\n",
    "                        .groupby('dst_id')\\\n",
    "                        .sum('rank_i')\\\n",
    "                        .withColumnRenamed('sum(rank_i)','rank_i')\n",
    "\n",
    "        # next we have to deal with dangling nodes and nodes that edges to unkown nodes not present in the graph\n",
    "        # in the end all of this gets also collected into alpha and distrbuted to all nodes in the graph\n",
    "        # full outer join because you dont want to lose nodes that either have no know input or no know output\n",
    "        ranks_df = ranks_df\\\n",
    "                    .join(ranks_one_df, ranks_df['uri_id'] == ranks_one_df['dst_id'],'outer')\\\n",
    "                    .drop('dst_id')\n",
    "\n",
    "        dangling_rank = ranks_df\\\n",
    "                        .filter(ranks_df.uri_id.isNull())\\\n",
    "                        .select(spark_sum('rank_i'))\\\n",
    "                        .first()[0]\n",
    "\n",
    "        ranks_df = ranks_df.filter(ranks_df.uri_id.isNotNull())\n",
    "\n",
    "        # because dangling is handeled like a link to every node dangling also needs to devalued by alpha!\n",
    "        if dangling_rank:\n",
    "            dist_alpha = ((dangling_rank/n_nodes)*(1-alpha)) + alpha\n",
    "        else:\n",
    "            dist_alpha = alpha\n",
    "        print(f'dangling sum:\\t{dangling_rank}')\n",
    "        print(f'alpha dist :\\t{dist_alpha}')\n",
    "\n",
    "        sum_alpha_and_pr_udf = udf(lambda x: (x * (1-alpha)) + dist_alpha, FloatType())\n",
    "        ranks_df = ranks_df.na.fill(0,['rank_i'])\n",
    "        ranks_df = ranks_df.withColumn('rank_i',sum_alpha_and_pr_udf('rank_i'))\n",
    "\n",
    "        # test for convergence\n",
    "        convergence_udf = udf(lambda rank_i,rank: abs(rank_i - rank) <= convergence, BooleanType())\n",
    "        ranks_df = ranks_df.withColumn('convergence',convergence_udf('rank','rank_i'))\n",
    "        count_not_converged = ranks_df.filter(ranks_df.convergence == False).count()\n",
    "        ranks_df = ranks_df.drop('convergence').drop('rank').withColumnRenamed('rank_i','rank')\n",
    "\n",
    "        # dataframe needs to be checkpointed here to truncate the logic path \n",
    "        # otherwise it would grow larger than the avalaible memory\n",
    "        ranks_df = ranks_df.checkpoint()\n",
    "\n",
    "        if count_not_converged == 0:\n",
    "            print(\"converged\")\n",
    "            break\n",
    "        else:\n",
    "            print(f'not yet converged {count_not_converged}')\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "    return ranks_df.drop('dst_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|src| dst|\n",
      "+---+----+\n",
      "|  6|   9|\n",
      "|  7|   6|\n",
      "|  5|   5|\n",
      "|  8|   7|\n",
      "|  7|   2|\n",
      "| 10|   8|\n",
      "|  3|   4|\n",
      "|  8|   8|\n",
      "|  1|  10|\n",
      "|  4|   8|\n",
      "| 17|null|\n",
      "+---+----+\n",
      "\n",
      "None\n",
      "##### Itteration:\t0 #####\n",
      "dangling sum:\t2.5\n",
      "alpha dist :\t0.38611111111111107\n",
      "not yet converged 9\n",
      "##### Itteration:\t1 #####\n",
      "dangling sum:\t1.602777749300003\n",
      "alpha dist :\t0.3013734541005558\n",
      "not yet converged 9\n",
      "##### Itteration:\t2 #####\n",
      "dangling sum:\t1.6317669451236725\n",
      "alpha dist :\t0.3041113225950135\n",
      "not yet converged 6\n",
      "##### Itteration:\t3 #####\n",
      "dangling sum:\t2.0793031752109528\n",
      "alpha dist :\t0.34637863321436774\n",
      "not yet converged 9\n",
      "##### Itteration:\t4 #####\n",
      "dangling sum:\t2.2273977994918823\n",
      "alpha dist :\t0.3603653477297889\n",
      "not yet converged 9\n",
      "##### Itteration:\t5 #####\n",
      "dangling sum:\t2.096063941717148\n",
      "alpha dist :\t0.3479615944955084\n",
      "not yet converged 7\n",
      "##### Itteration:\t6 #####\n",
      "dangling sum:\t1.9981269836425781\n",
      "alpha dist :\t0.33871199289957676\n",
      "not yet converged 6\n",
      "##### Itteration:\t7 #####\n",
      "dangling sum:\t1.970376044511795\n",
      "alpha dist :\t0.3360910708705584\n",
      "not yet converged 4\n",
      "##### Itteration:\t8 #####\n",
      "dangling sum:\t1.971101313829422\n",
      "alpha dist :\t0.3361595685283343\n",
      "not yet converged 3\n",
      "##### Itteration:\t9 #####\n",
      "dangling sum:\t1.965386301279068\n",
      "alpha dist :\t0.33561981734302304\n",
      "not yet converged 3\n",
      "##### Itteration:\t10 #####\n",
      "dangling sum:\t1.9517143368721008\n",
      "alpha dist :\t0.33432857626014284\n",
      "not yet converged 1\n",
      "##### Itteration:\t11 #####\n",
      "dangling sum:\t1.9389061033725739\n",
      "alpha dist :\t0.3331189097629653\n",
      "not yet converged 1\n",
      "##### Itteration:\t12 #####\n",
      "dangling sum:\t1.9301502108573914\n",
      "alpha dist :\t0.33229196435875363\n",
      "not yet converged 1\n",
      "##### Itteration:\t13 #####\n",
      "dangling sum:\t1.924095630645752\n",
      "alpha dist :\t0.331720142894321\n",
      "not yet converged 1\n",
      "##### Itteration:\t14 #####\n",
      "dangling sum:\t1.9190734326839447\n",
      "alpha dist :\t0.3312458241979281\n",
      "not yet converged 1\n",
      "##### Itteration:\t15 #####\n",
      "dangling sum:\t1.9146175682544708\n",
      "alpha dist :\t0.3308249925573667\n",
      "not yet converged 1\n",
      "##### Itteration:\t16 #####\n",
      "dangling sum:\t1.9108432829380035\n",
      "alpha dist :\t0.3304685322774781\n",
      "converged\n",
      "+------+----------+\n",
      "|uri_id|      rank|\n",
      "+------+----------+\n",
      "|     7| 1.3469086|\n",
      "|     6|0.90386575|\n",
      "|    17|0.33046854|\n",
      "|     5| 2.1465194|\n",
      "|     1|0.33046854|\n",
      "|    10| 0.6116698|\n",
      "|     3|0.33046854|\n",
      "|     8| 2.3879611|\n",
      "|     4| 0.6116698|\n",
      "+------+----------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    input_base_path = '../data/results/'\n",
    "    webgraph_path     = input_base_path + 'webgraph_v2_without_null/'\n",
    "    pagerank_out_path = input_base_path + 'pagerank_df_parquet/'\n",
    "    \n",
    "    \n",
    "    random_graph_size = 10\n",
    "    graph_path = None\n",
    "    output_path = None\n",
    "    chekpoint_dir ='../data/checkpoints'\n",
    "    \n",
    "    spark.sparkContext.setCheckpointDir(chekpoint_dir)\n",
    "    \n",
    "    if graph_path:\n",
    "        graph_df = spark.read.parquet(webgraph_path).cache()\n",
    "    else:\n",
    "        graph_df = spark\\\n",
    "                    .createDataFrame(create_random_graph(random_graph_size))\\\n",
    "                    .withColumnRenamed('_1','src')\\\n",
    "                    .withColumnRenamed('_2','dst')\n",
    "        \n",
    "        print(graph_df.show())\n",
    "        \n",
    "    ranked_graph_df = run(graph_df)\n",
    "    print(ranked_graph_df.show())\n",
    "    \n",
    "    if output_path:\n",
    "        ranked_graph_df.write.parquet(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
